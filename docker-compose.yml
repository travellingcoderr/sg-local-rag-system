# Local dev: app + Ollama. App uses Ollama container for LLM.
# Prod: run the app elsewhere and set LLM_PROVIDER=openai or gemini + API keys (no Ollama service).
services:
  app:
    build: .
    ports:
      - "8501:8501"
    environment:
      - LLM_PROVIDER=ollama
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-llama3.2:1b}
    env_file:
      - .env
    depends_on:
      - ollama
    volumes:
      # Persist logs and any uploaded/embedding data if needed
      - ./logs:/app/logs

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Optional: pull a model on first run
    # command: ["serve"]
    # Then run: docker compose exec ollama ollama pull llama3.2:1b

volumes:
  ollama_data:
